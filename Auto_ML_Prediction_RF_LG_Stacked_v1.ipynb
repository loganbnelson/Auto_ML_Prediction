{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0d96db",
   "metadata": {},
   "source": [
    "# Let's predict an output value! This will walk you through the process of loading training and test data and get predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a2919",
   "metadata": {},
   "source": [
    "## Created by Logan Nelson\n",
    "See github for licensing. https://github.com/loganbnelson/Auto_ML_Prediction\n",
    "### logan.b.nelson@gmail.com, nelso566@purdue.edu, 818-925-6426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the packages, uncomment the below to install\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b45d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules required to run everything\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc9a74",
   "metadata": {},
   "source": [
    "Let's set up a few funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25186daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sets of data using the following function\n",
    "\n",
    "def read_csv_as_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Read a CSV file and return its data as a pandas DataFrame.\n",
    "\n",
    "    Arguments to enter:\n",
    "    file_path (str): The path to the CSV file. This will default to the current working directory.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the CSV data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Let's define a function to split our data into X and y portions. We'll split into k-folds during the actual model fit.\n",
    "# At this point the x vs y can be defined by column name. If none are defined it will default to \"SalePrice.\"\n",
    "# I am temporarily removing the input side of this function for the purpose of this assignment. Recommend adding in for\n",
    "# other applications.\n",
    "\n",
    "def split_data(train, test):\n",
    "    \"\"\"\n",
    "    Split a dataset into X_train, y_train, X_test, and y_test.\n",
    "\n",
    "    Args:\n",
    "    train (pd.DataFrame): The training dataset.\n",
    "    test (pd.DataFrame): The testing dataset.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: X_train (features for training).\n",
    "    pd.Series: y_train (target for training).\n",
    "    pd.DataFrame: X_test (features for testing).\n",
    "    pd.Series or None: y_test (target for testing or None if not available).\n",
    "    \"\"\"\n",
    "    print(\"\\nHere are the column names to be selected from the train set.\")\n",
    "    print(train.columns)\n",
    "    y_train_col = input(\"\\n\\nWhat column is the output or y for Training?: \") or \"SalePrice\"\n",
    "    \n",
    "    if y_train_col not in train.columns:\n",
    "        raise ValueError(f\"'{y_train_col}' not found in the training dataset columns.\")\n",
    "    \n",
    "    print(\"\\n\\nHere are the column names to be selected from the test set.\")\n",
    "    print(test.columns)\n",
    "    y_test_col = input(\"\\n\\nWhat column is the output or y for testing?: \") or y_train_col\n",
    "    \n",
    "    def extract_numeric(series):\n",
    "        # Use regular expression to extract numeric parts of the values\n",
    "        return series.apply(lambda x: float(''.join(re.findall(r'\\d+\\.?\\d*', str(x)))))\n",
    "    \n",
    "    try:\n",
    "        X_train = train.drop(y_train_col, axis=1)\n",
    "        y_train = extract_numeric(train[y_train_col])\n",
    "    except:\n",
    "        raise ValueError(\"\\n\\nThe Train set was unable to be split. Did you select a valid column? The Train set must contain a valid y column.\")\n",
    "    \n",
    "    try:\n",
    "        X_test = test.drop(y_test_col, axis=1)\n",
    "        y_test = extract_numeric(test[y_test_col])\n",
    "    except:\n",
    "        X_test = test\n",
    "        print(f\"\\n\\nThe Test does not contain numeric values in '{y_test_col}'. Did you select a valid column? If the Test set does not contain the y column, y_test will be set to None.\")\n",
    "        y_test = None  # Set y_test to None if it's not available\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Let's set up a funciton to split data if only one file is submitted. It must contain all Y output values througout.\n",
    "\n",
    "def split_dataframe(data, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into train and test DataFrames.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The input DataFrame to be split.\n",
    "    test_size (float, optional): The proportion of the data to include in the test split (between 0.0 and 1.0).\n",
    "    random_state (int or None, optional): Seed for random number generation (for reproducibility).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The training DataFrame.\n",
    "    pd.DataFrame: The testing DataFrame.\n",
    "    \"\"\"\n",
    "    if test_size < 0.0 or test_size > 1.0:\n",
    "        raise ValueError(\"test_size should be a float between 0.0 and 1.0.\")\n",
    "\n",
    "    # Split the DataFrame into train and test sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def select_or_create_column(dataframe, prompt=\"Select the appropriate column as index. If no column represents the index just hit Enter:  \", default_column=None):\n",
    "    \"\"\"\n",
    "    Prompt the user to select an existing column or create a new one with sequential values.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The DataFrame to work with.\n",
    "    prompt (str, optional): The prompt message for the user. Default is \"Select or create a column:\".\n",
    "    default_column (str, optional): The default column to use if the user doesn't specify one. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: The selected or created column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the column names in the DataFrame\n",
    "    print(\"Available columns:\")\n",
    "    print(dataframe.columns)\n",
    "\n",
    "    # Ask the user for their choice\n",
    "    user_input = input(f\"{prompt} \")\n",
    "\n",
    "    if not user_input.strip():\n",
    "        # If the user didn't provide a column name, use the default_column or create a new one\n",
    "        if default_column:\n",
    "            selected_column = dataframe[default_column]\n",
    "        else:\n",
    "            # Create a new column with sequential values\n",
    "            new_column_name = input(\"\"\"Enter a name for the new column. \n",
    "            It can be the same column name as previous or can be a new column name: \"\"\")\n",
    "            if not new_column_name:\n",
    "                raise ValueError(\"A name is required for the new column.\")\n",
    "            \n",
    "            # Generate sequential values for the new column\n",
    "            selected_column = pd.Series(range(len(dataframe)), name=new_column_name)\n",
    "    else:\n",
    "        # Use the user's input as the column name\n",
    "        if user_input in dataframe.columns:\n",
    "            selected_column = dataframe[user_input]\n",
    "        else:\n",
    "            raise ValueError(f\"'{user_input}' is not a valid column in the DataFrame.\")\n",
    "\n",
    "    return selected_column\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "# selected_col = select_or_create_column(df, default_column='A')\n",
    "# print(selected_col)\n",
    "\n",
    "\n",
    "# Let's handle categorical data by creating one-hot encoding. We'll do this for both X train and test sets.\n",
    "\n",
    "def encode_categorical_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encode categorical features using one-hot encoding with consistent columns.\n",
    "\n",
    "    Args:\n",
    "    X_train (pd.DataFrame): The feature matrix for the training data.\n",
    "    X_test (pd.DataFrame): The feature matrix for the testing data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The encoded training data with consistent columns.\n",
    "    pd.DataFrame: The encoded testing data with consistent columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine train and test sets to find common unique categories\n",
    "    combined_data = pd.concat([X_train, X_test])\n",
    "\n",
    "    # Identify categorical features based on data types\n",
    "    categorical_features = combined_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Apply one-hot encoding to combined data\n",
    "    combined_data_encoded = pd.get_dummies(combined_data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    # Split the combined data back into train and test sets\n",
    "    X_train_encoded = combined_data_encoded.iloc[:len(X_train)]\n",
    "    X_test_encoded = combined_data_encoded.iloc[len(X_train):]\n",
    "\n",
    "    # Ensure that both train and test sets have the same columns\n",
    "    # If a column is missing in one set, add it with zeros\n",
    "    for column in X_train_encoded.columns:\n",
    "        if column not in X_test_encoded.columns:\n",
    "            X_test_encoded[column] = 0\n",
    "\n",
    "    for column in X_test_encoded.columns:\n",
    "        if column not in X_train_encoded.columns:\n",
    "            X_train_encoded[column] = 0\n",
    "\n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Let's define a simple linear regression to get results of the train data set. It will be run K_folds number of times.\n",
    "\n",
    "def train_simple_linear_regression_kfold(X_train, y_train, X_test, k_folds=5):\n",
    "    \"\"\"\n",
    "    Train a simple linear regression model with k-fold cross-validation.\n",
    "\n",
    "    Args:\n",
    "    X_train (pd.DataFrame): The feature matrix for training data.\n",
    "    y_train (pd.Series): The target vector for training data.\n",
    "    X_test (pd.DataFrame): The feature matrix for testing data.\n",
    "    k_folds (int, optional): The number of folds for k-fold cross-validation. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    sklearn.linear_model.LinearRegression: The trained linear regression model.\n",
    "    float: The average mean squared error (MSE) from k-fold cross-validation.\n",
    "    float: The average coefficient of determination (R-squared) from k-fold cross-validation.\n",
    "    pd.Series: The predictions from the best-performing fold (according to validation set).\n",
    "    pd.Series: The y predictors from the best-performing fold.\n",
    "    np.ndarray: The feature importances from the best-performing fold.\n",
    "    \"\"\"\n",
    "\n",
    "    if k_folds <= 1:\n",
    "        raise ValueError(\"Number of folds (k_folds) must be greater than 1.\")\n",
    "\n",
    "    # Initialize variables for k-fold metrics and predictions\n",
    "    mse_values = []\n",
    "    r2_values = []\n",
    "    best_fold_mse = float('inf')\n",
    "    best_fold_r2 = -float('inf')\n",
    "    best_fold_predictions = None\n",
    "    best_fold_y_predictors = None\n",
    "    best_fold_importances = None\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=None)\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Create a linear regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Train the model on the training fold\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Make predictions on the validation fold\n",
    "        y_pred_val = model.predict(X_val_fold)\n",
    "\n",
    "        # Calculate mean squared error (MSE) and R-squared for validation fold\n",
    "        mse_fold = mean_squared_error(y_val_fold, y_pred_val)\n",
    "        r2_fold = r2_score(y_val_fold, y_pred_val)\n",
    "\n",
    "        # Update best fold metrics and predictions if necessary\n",
    "        if mse_fold < best_fold_mse:\n",
    "            best_fold_mse = mse_fold\n",
    "            best_fold_r2 = r2_fold\n",
    "            best_fold_predictions = model.predict(X_train)  # Predict on X_train\n",
    "            best_fold_y_predictors = model.predict(X_test) # Predict on X_test\n",
    "            best_fold_importances = np.array([0])\n",
    "\n",
    "        # Append fold metrics to lists\n",
    "        mse_values.append(mse_fold)\n",
    "        r2_values.append(r2_fold)\n",
    "\n",
    "    # Calculate average validation metrics\n",
    "    avg_mse_val = (sum(mse_values) / k_folds)\n",
    "    avg_r2_val = (sum(r2_values) / k_folds)\n",
    "\n",
    "    return model, avg_mse_val, avg_r2_val, best_fold_predictions, best_fold_y_predictors, best_fold_importances\n",
    "\n",
    "# Let's define a Random Forest model to get results of the train data set. It will be run K_folds number of times.\n",
    "\n",
    "def train_random_forest_regression(X_train, y_train, X_test=None, n_estimators=100, max_depth=None, min_samples_split=2\n",
    "                                   , min_samples_leaf=1, k_folds=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Train a regression-enhanced random forest model with k-fold cross-validation on the training data.\n",
    "\n",
    "    Args:\n",
    "    X_train (pd.DataFrame): The feature matrix for training data.\n",
    "    y_train (pd.Series): The target vector for training data.\n",
    "    X_test (pd.DataFrame, optional): The feature matrix for testing data. Default is None.\n",
    "    n_estimators (int, optional): The number of trees in the forest. Default is 100.\n",
    "    max_depth (int or None, optional): The maximum depth of the tree. None means no maximum depth. Default is None.\n",
    "    min_samples_split (int, optional): The minimum number of samples required to split an internal node. Default is 2.\n",
    "    min_samples_leaf (int, optional): The minimum number of samples required to be at a leaf node. Default is 1.\n",
    "    k_folds (int, optional): The number of folds for k-fold cross-validation. Default is 5.\n",
    "    random_state (int or None, optional): The random seed for reproducibility. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    sklearn.ensemble.RandomForestRegressor: The trained random forest regression model.\n",
    "    float: The mean squared error (MSE) from k-fold cross-validation on the training data.\n",
    "    float: The coefficient of determination (R-squared) from k-fold cross-validation on the training data.\n",
    "    pd.Series: The predicted values on the training data using the best fold.\n",
    "    pd.Series: The predicted values on the test data (if provided).\n",
    "    np.array: The feature importances of the best fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a random forest regression model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Initialize variables for validation metrics and predictions\n",
    "    mse_val = []\n",
    "    r2_val = []\n",
    "    y_pred_train = []\n",
    "    best_mse = float('inf')  # Initialize with a high value\n",
    "    best_fold_predictions = None\n",
    "    best_fold_importances = None\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Make predictions on the validation fold\n",
    "        y_pred_val = model.predict(X_val_fold)\n",
    "        y_pred_train.extend(y_pred_val)  # Collect predicted values\n",
    "\n",
    "        # Calculate mean squared error (MSE) and R-squared for validation fold\n",
    "        mse_fold = mean_squared_error(y_val_fold, y_pred_val)\n",
    "        r2_fold = r2_score(y_val_fold, y_pred_val)\n",
    "\n",
    "        # Append validation metrics\n",
    "        mse_val.append(mse_fold)\n",
    "        r2_val.append(r2_fold)\n",
    "\n",
    "        # Track the fold with the lowest MSE (or other metric)\n",
    "        if mse_fold < best_mse:\n",
    "            best_mse = mse_fold\n",
    "            best_fold_predictions = model.predict(X_train)\n",
    "            best_fold_importances = model.feature_importances_\n",
    "            best_r2_val = r2_fold\n",
    "        # If test data is provided, evaluate the model on the test data\n",
    "            if X_test is not None:\n",
    "                y_pred_test = model.predict(X_test)\n",
    "            else:\n",
    "                y_pred_test = None\n",
    "    \n",
    "    # Calculate average validation metrics\n",
    "    avg_mse_val = (sum(mse_val) / k_folds)\n",
    "    avg_r2_val = (sum(r2_val) / k_folds)\n",
    "\n",
    "    return model, best_mse, best_r2_val, pd.Series(best_fold_predictions), pd.Series(y_pred_test), best_fold_importances\n",
    "\n",
    "# Let's created a stacked model based on all the models created.\n",
    "\n",
    "def stack_models(base_models, base_predictions, X_holdout, y_holdout, X_test, meta_model=LinearRegression()):\n",
    "    \"\"\"\n",
    "    Stack (combine) multiple base models using a meta-model and calculate MSE and R2.\n",
    "\n",
    "    Args:\n",
    "    base_models (list): List of base models (already trained).\n",
    "    base_predictions (list): List of predicted values from the base models.\n",
    "    meta_model (sklearn model): The meta-model (e.g., Linear Regression) to combine base model predictions.\n",
    "    X_holdout (pd.DataFrame): Feature matrix of the holdout set.\n",
    "    y_holdout (pd.Series): Target vector of the holdout set.\n",
    "    X_test (pd.DataFrame): Feature matrix of the test set.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: Predicted values on the test set using the stacked model.\n",
    "    float: Mean Squared Error (MSE) of the stacked model on the holdout set.\n",
    "    float: R-squared (R2) of the stacked model on the holdout set.\n",
    "    \"\"\"\n",
    "    # Combine predicted values from base models into a new feature matrix\n",
    "    X_meta = np.column_stack(base_predictions)\n",
    "\n",
    "    # Train the meta-model on the combined predictions of the holdout set\n",
    "    meta_model.fit(X_meta, y_holdout)\n",
    "\n",
    "    # Use base models to predict on the test data\n",
    "    base_predictions_test = [model.predict(X_test) for model in base_models]\n",
    "\n",
    "    # Combine test data predictions into a new feature matrix\n",
    "    X_meta_test = np.column_stack(base_predictions_test)\n",
    "\n",
    "    # Use the meta-model to make final predictions on the test data\n",
    "    final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "    # Calculate MSE and R2 on the holdout set\n",
    "    y_pred_holdout = meta_model.predict(X_meta)\n",
    "    mse_holdout = mean_squared_error(y_holdout, y_pred_holdout)\n",
    "    r2_holdout = r2_score(y_holdout, y_pred_holdout)\n",
    "\n",
    "    return final_predictions, mse_holdout, r2_holdout\n",
    "\n",
    "# If needed, repalce NA with Nulls in the data sets.\n",
    "\n",
    "def replace_na_with_nulls(data):\n",
    "    \"\"\"\n",
    "    Replace all \"NA\" string values in a DataFrame with nulls (NaN).\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The DataFrame in which to replace string \"NA\" values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with \"NA\" values replaced by nulls.\n",
    "    \"\"\"\n",
    "    # Replace \"NA\" values with NaN\n",
    "    data = data.replace(\"NA\", pd.NA)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Imputate data as means for Null values. Changing this to median for this iteration.\n",
    "\n",
    "def impute_missing_values(data, numeric_strategy='median', categorical_strategy='constant', constant_value=None):\n",
    "    \"\"\"\n",
    "    Impute missing values in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The DataFrame with missing values to be imputed.\n",
    "    numeric_strategy (str, optional): Imputation strategy for numerical features ('mean', 'median', or 'mode'). \n",
    "        Default is 'mean'.\n",
    "    categorical_strategy (str, optional): Imputation strategy for categorical features ('constant' or 'mode'). \n",
    "        Default is 'constant'.\n",
    "    constant_value (any, optional): The constant value to use for categorical imputation. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    imputed_data = data.copy()\n",
    "\n",
    "    # Impute missing values for numeric features\n",
    "    numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_columns:\n",
    "        if imputed_data[col].isna().any():\n",
    "            if numeric_strategy == 'mean':\n",
    "                imputed_data[col].fillna(data[col].mean(), inplace=True)\n",
    "            elif numeric_strategy == 'median':\n",
    "                imputed_data[col].fillna(data[col].median(), inplace=True)\n",
    "            elif numeric_strategy == 'mode':\n",
    "                imputed_data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "\n",
    "    # Impute missing values for categorical features\n",
    "    categorical_columns = data.select_dtypes(exclude=['number']).columns\n",
    "    for col in categorical_columns:\n",
    "        if imputed_data[col].isna().any():\n",
    "            if categorical_strategy == 'constant':\n",
    "                imputed_data[col].fillna(constant_value, inplace=True)\n",
    "            elif categorical_strategy == 'mode':\n",
    "                imputed_data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "\n",
    "    return imputed_data\n",
    "\n",
    "# Ensure that Train and Test (or any two dataframes) sets contain the same columns. Essentially an inner join. Un-used for now.\n",
    "\n",
    "def align_columns(df1, df2):\n",
    "    \"\"\"\n",
    "    Align the columns of two DataFrames based on a common set of columns.\n",
    "\n",
    "    Args:\n",
    "    df1 (pd.DataFrame): The first DataFrame.\n",
    "    df2 (pd.DataFrame): The second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame 1 with columns aligned.\n",
    "    pd.DataFrame: DataFrame 2 with columns aligned.\n",
    "    \"\"\"\n",
    "    common_columns = list(set(df1.columns) & set(df2.columns))\n",
    "\n",
    "    df1_aligned = df1[common_columns]\n",
    "    df2_aligned = df2[common_columns]\n",
    "\n",
    "    return df1_aligned, df2_aligned\n",
    "\n",
    "# Write lists or dataframes desired to csv.\n",
    "\n",
    "def write_to_csv(data, file_name):\n",
    "    \"\"\"\n",
    "    Write data (list or DataFrame) to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    data (list or pd.DataFrame): The data to be written to the CSV file.\n",
    "    file_name (str): The name of the CSV file (including the .csv extension).\n",
    "\n",
    "    Returns:\n",
    "    None, but creates CSV file.\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):\n",
    "        # If data is a list, convert it to a DataFrame\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    pd.DataFrame(data).to_csv(file_name, index=False)\n",
    "    print(f\"Data has been written to {file_name}\")\n",
    "    \n",
    "# Get a histogram of results to visualize a set of values. Could be useful to compare predictions vs y train data.\n",
    "\n",
    "def plot_histogram(data, column_name=None, bins=10, title=\"Histogram\"):\n",
    "    \"\"\"\n",
    "    Generate a histogram plot for a list, NumPy array, DataFrame with one column, or a specific column of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    data (list, np.ndarray, pd.DataFrame, pd.Series): The data to create a histogram for.\n",
    "    column_name (str, optional): If data is a DataFrame, specify the column name to plot. Default is None.\n",
    "    bins (int, optional): The number of bins for the histogram. Default is 10.\n",
    "    title (str, optional): The title for the histogram plot. Default is \"Histogram\".\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Handling for different data types (list, NumPy array, DataFrame, Series)\n",
    "    if isinstance(data, (list, np.ndarray)):\n",
    "        plt.hist(data, bins=bins)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        if column_name is not None:\n",
    "            if column_name in data.columns:\n",
    "                plt.hist(data[column_name], bins=bins)\n",
    "            else:\n",
    "                print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "                return\n",
    "        else:\n",
    "            print(\"Please specify a column name for DataFrame input.\")\n",
    "            return\n",
    "    elif isinstance(data, pd.Series):\n",
    "        plt.hist(data, bins=bins)\n",
    "    else:\n",
    "        print(\"Unsupported data type. Please provide a list, NumPy array, DataFrame, or Series.\")\n",
    "        return\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "\n",
    "    # Show the histogram plot\n",
    "    plt.show()\n",
    "\n",
    "# Comined two arguments that are Single-column DF, single-row DF, arrays, lists, or series into a single DF with two columns.    \n",
    "    \n",
    "def combine_to_dataframe(arg1, arg2, column_names=('Column 1', 'Column 2')):\n",
    "    \"\"\"\n",
    "    Combine two arguments (single-column DataFrames, single-row DataFrames, NumPy arrays, lists, or Pandas Series) into a\n",
    "    single DataFrame with two columns.\n",
    "\n",
    "    Args:\n",
    "    arg1, arg2 (pd.DataFrame, np.ndarray, list, pd.Series): The two arguments to combine.\n",
    "    column_names (tuple, optional): A tuple containing the names for the two columns. Default is ('Column 1', 'Column 2').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with two columns.\n",
    "    \"\"\"\n",
    "    # Convert arguments to DataFrames if they are not already\n",
    "    if not isinstance(arg1, (pd.DataFrame, pd.Series)):\n",
    "        arg1 = pd.DataFrame(arg1, columns=[column_names[0]])\n",
    "    if not isinstance(arg2, (pd.DataFrame, pd.Series)):\n",
    "        arg2 = pd.DataFrame(arg2, columns=[column_names[1]])\n",
    "\n",
    "    # Combine the two DataFrames side by side\n",
    "    combined_df = pd.concat([arg1, arg2], axis=1)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# A function to evaluate all R2 values from models and return the best predicted results based on the training data.\n",
    "\n",
    "def select_best_model_and_predictions(r2_values, predictions_list, model_names):\n",
    "    \"\"\"\n",
    "    Select the best-performing model based on R2 values and return its predicted values.\n",
    "\n",
    "    Args:\n",
    "    r2_values (list): List of R2 values for each model.\n",
    "    predictions_list (list): List of predicted values (pd.Series) from different models.\n",
    "    model_names (list): List of model names or identifiers corresponding to each R2 value and prediction.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: Predicted values from the best-performing model.\n",
    "    str: Name or identifier of the best-performing model.\n",
    "    \"\"\"\n",
    "\n",
    "    if not (r2_values and predictions_list and model_names):\n",
    "        raise ValueError(\"No R2 values, predicted values, or model names provided.\")\n",
    "\n",
    "    if len(r2_values) != len(predictions_list) or len(r2_values) != len(model_names):\n",
    "        raise ValueError(\"Inconsistent number of R2 values, predicted values, or model names provided.\")\n",
    "\n",
    "    # Find the index of the model with the highest R2\n",
    "    best_model_index = max(range(len(r2_values)), key=lambda i: r2_values[i])\n",
    "\n",
    "    # Get the predicted values from the best-performing model\n",
    "    best_predictions = predictions_list[best_model_index]\n",
    "    best_model_name = model_names[best_model_index]\n",
    "    best_r2 = r2_values[best_model_index]\n",
    "\n",
    "    return best_predictions, best_model_name, best_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff211d5",
   "metadata": {},
   "source": [
    "The following executes functions according to user input. It should work for any tabular data set(s) in csv form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6035236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Configuration\n",
    "INPUT_DATA_IS_SPLIT = input(\"Is your data already split into a Train and Test set? Y or N: \").upper() or \"N\"\n",
    "if INPUT_DATA_IS_SPLIT == \"N\":\n",
    "    DATA_CSV_PATH = input(\"\"\"Please enter the name of the file if it is in the same folder as this program.\n",
    "    If it is not in the same folder, please type the file path with the file name.   \"\"\")\n",
    "else:\n",
    "    TRAIN_CSV_PATH = input(\"\"\"Please enter the name of the file for your TRAIN dataset. \n",
    "    If it is in the same folder as this program, just enter the name.\n",
    "    If it is not in the same folder, please type the file path with the file name. \"\"\")\n",
    "    TEST_CSV_PATH = input(\"\"\"Please enter the name of the file for your TRAIN dataset. \n",
    "    If it is in the same folder as this program, just enter the name.\n",
    "    If it is not in the same folder, please type the file path with the file name. \"\"\")\n",
    "\n",
    "N_ESTIMATORS = int(input(\"How many estimators would you like to use? Suggested between 64 and 128.   \") or 64)\n",
    "# MAX_DEPTH = int(input(\"What Max depth would you like for the forest model? E.g. None or 1+   \") or 0) # Would love to make this work, but on hold for now.\n",
    "MAX_DEPTH = None\n",
    "MIN_SAMPLES_SPLIT = int(input(\"What mimimum samples do you want for the random forest splits?   \") or 2)\n",
    "MIN_SAMPLES_LEAF = int(input(\"What mimimum samples do you want for each leaf for the random forest?   \") or 1)\n",
    "K_FOLDS = int(input(\"How many folds would you like for the cross validation?   \") or 20)\n",
    "RANDOM_STATE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223746db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use read_csv_as_dataframe to read in the csvs needed as dataframes\n",
    "if INPUT_DATA_IS_SPLIT == \"N\":\n",
    "    try: combined_data = read_csv_as_dataframe(DATA_CSV_PATH)\n",
    "    except: print(\"The file cannot be found. Please check the file path and/or name and try again.\")\n",
    "    try: train_data, test_data = split_dataframe(combined_data, test_size=0.2, random_state=None)\n",
    "    except: print(\"\"\"The data file was found, but is unable to be split into train and test sets. \n",
    "    Please check the data to confirm it is structured as a delimited file.\"\"\")\n",
    "else:\n",
    "    train_data = read_csv_as_dataframe(TRAIN_CSV_PATH)\n",
    "    test_data = read_csv_as_dataframe(TEST_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values to handle Null or NaN entries.\n",
    "\n",
    "train_data = impute_missing_values(train_data, numeric_strategy='median', categorical_strategy='constant', constant_value=0)\n",
    "test_data = impute_missing_values(test_data, numeric_strategy='median', categorical_strategy='constant', constant_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 rows for the train data.\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the test and train sets now. Storing the result in a tuple for future use.\n",
    "# For this competition you can just hit enter without any value as it is coded for the data used. \n",
    "# For other purposes enter the column that is the output of the training and test set.\n",
    "\n",
    "data_split = split_data(train_data, test_data)\n",
    "\n",
    "X_test_encoded_index = select_or_create_column(data_split[2], prompt=\"Select or create a column:\", default_column=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83808f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Training and Testing Data\n",
    "X_train_not_encoded, y_train, X_test_not_encoded, y_test = data_split\n",
    "\n",
    "# Encode Categorical Features\n",
    "X_train_encoded, X_test_encoded = encode_categorical_features(X_train_not_encoded, X_test_not_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01149d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the shape of our split datasets.\n",
    "\n",
    "print(f\"X_train_encoded shape: {X_train_encoded.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test_encoded shape: {X_test_encoded.shape}\")\n",
    "\n",
    "if y_test is not None:\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "else:\n",
    "    print(\"y_test is None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e026d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get the random forest model results and store them.\n",
    "\n",
    "train_model_results = train_random_forest_regression(\n",
    "    X_train_encoded, y_train, X_test_encoded, n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, min_samples_split=MIN_SAMPLES_SPLIT\n",
    "    , min_samples_leaf=MIN_SAMPLES_LEAF\n",
    "    , k_folds=K_FOLDS, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results of our forest model and a small subset of the values.\n",
    "\n",
    "print(\"The model used is \" + str(train_model_results[0]) + \". Here are\"\n",
    "      + \" the best mse test is: \" + str(train_model_results[1]) + \". The best R2 of the model is \" + \n",
    "     str(train_model_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get a simple model results and store them. This will be a linear regression.\n",
    "\n",
    "simple_model_results = train_simple_linear_regression_kfold(X_train_encoded, y_train, X_test_encoded, k_folds=K_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results of our model and a small subset of the values.\n",
    "\n",
    "print(\"The model used is \" + str(simple_model_results[0]) + \". Here are\"\n",
    "      + \" the best mse test is: \" + str(simple_model_results[1]) + \". The best R2 of the model is \" +\n",
    "    str(simple_model_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da56fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we call the stacked model function to get a combined value for all the moodel results we've gotten so far.\n",
    "\n",
    "stacked_model = stack_models([train_model_results[0], simple_model_results[0]],\n",
    "                             [train_model_results[3], simple_model_results[3]],\n",
    "                             X_train_encoded, y_train, X_test_encoded, meta_model=LinearRegression())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b231a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the Linear Regression results to the Random Forest and give the best model.\n",
    "\n",
    "best_y_predictions = select_best_model_and_predictions([train_model_results[2], simple_model_results[2], stacked_model[2]], \n",
    "                                                       [train_model_results[4], simple_model_results[4], stacked_model[0]],\n",
    "                                                       [train_model_results[0], simple_model_results[0], \"Stacked Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b14fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Best Model Results\n",
    "best_model_index = best_y_predictions[1]\n",
    "best_predictions = best_y_predictions[0]\n",
    "best_r2 = best_y_predictions[2]\n",
    "\n",
    "print(f\"The best model used is: {best_model_index}.\")\n",
    "print(f\"The R-squared (R2) of the best model is: {best_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d502b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the combination of House ID and Predicted Sale Price from the best model now and store it as a dataframe.\n",
    "\n",
    "best_predictions_df = combine_to_dataframe(X_test_encoded_index, best_y_predictions[0], column_names=(X_test_encoded_index.name, data_split[1].name))\n",
    "\n",
    "# Display the resulting DataFrame.\n",
    "best_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we write it to a csv so we can hand off the results to any who want to use it.\n",
    "\n",
    "write_to_csv(best_predictions_df, 'best_y_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the Histogram of the prices predicted. \n",
    "# It will be useful to compare to the prices of the train set to see if it makes some sense.\n",
    "\n",
    "plot_title = str((\"Histogram of Predicted \" + data_split[1].name))\n",
    "plot_histogram(best_y_predictions[0], bins=20, title=plot_title)\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the Histogram of the prices from the train data. \n",
    "\n",
    "plot_title = str((\"Histogram of Training \" + y_train.name))\n",
    "plot_histogram(y_train, bins=20, title=plot_title)\n",
    "plt.show()  # Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
